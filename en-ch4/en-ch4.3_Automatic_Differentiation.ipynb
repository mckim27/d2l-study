{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 4 - THE PRELIMINARIES: A CRASHCOURSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Automatic Differentiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.1 A simple Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a toy example, say that we are interested in differentiating the mapping **y = 2x⊤x** with respect to the column vector x.<br> \n",
    "  To start, let’s create the variable x and assign it an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 2. 3.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[28.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * nd.dot(x, x)\n",
    "y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient of the function **y = 2x⊤x** with respect to x should be **_4x_**.<br> \n",
    "  Now let’s verify that the gradient produced is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[ 0.  4.  8. 12.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)\n",
    "print(x.grad - 4 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.         0.26726124 0.5345225  0.80178374]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.norm()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.2 Backward for Non-scalar Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x vector :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "y vector :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "v scalar :  \n",
      "[14.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "u.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('x vector : ', x)\n",
    "\n",
    "with autograd.record(): # y is a vector\n",
    "    y = x * x\n",
    "print('y vector : ', y)    \n",
    "y.backward()\n",
    "print('x.grad : ', x.grad)\n",
    "\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "\n",
    "with autograd.record(): # v is scalar\n",
    "    v = (u * u).sum()\n",
    "print('v scalar : ', v)    \n",
    "v.backward()\n",
    "print('u.grad : ', u.grad)\n",
    "\n",
    "x.grad - u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.3 Detach Computations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "z :  \n",
      "[ 0.  1.  8. 27.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "print('x : ', x)\n",
    "print('u : ', u)\n",
    "print('z : ', z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x.grad : ', x.grad)\n",
    "print('u : ', u)\n",
    "\n",
    "x.grad - u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following backward computes **_∂u2x/∂x_** with u = x instead of **∂x3/∂x**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the computation of y is still recorded, we can call y.backward() to get **∂y/∂x = 2x**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "print('y : ', y)\n",
    "print('x.grad : ', x.grad)\n",
    "print('x : ', x)\n",
    "\n",
    "x.grad - 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.4 Attach Gradients to Internal Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 1. 1. 1.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [1. 1. 1. 1.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [0. 0. 0. 0.]\n",
       " <NDArray 4 @cpu(0)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.ones(4) * 2 \n",
    "y.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    u.attach_grad() # implicitly run u = u.detach()\n",
    "    z = u + x \n",
    "    \n",
    "z.backward() \n",
    "x.grad, u.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.5 Head gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 1. 1. 1.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [0. 0. 0. 0.]\n",
       " <NDArray 4 @cpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.ones(4) * 2 \n",
    "y.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    v = u.detach() # u still keeps the computation graph\n",
    "    v.attach_grad()\n",
    "    z = v + x \n",
    "    \n",
    "z.backward() \n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [2. 2. 2. 2.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [0. 1. 2. 3.]\n",
       " <NDArray 4 @cpu(0)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.backward(v.grad) \n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.6 Computing the Gradient of Python Control Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    \n",
    "    while b.norm().asscalar() < 1000:\n",
    "        b = b * 2\n",
    "        \n",
    "    if b.sum().asscalar() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "        \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random.normal(shape=1) \n",
    "a.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    d = f(a) \n",
    "    \n",
    "d.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  \n",
      "[1.1630787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "d :  \n",
      "[1190.9926]\n",
      "<NDArray 1 @cpu(0)>\n",
      "a.grad :  \n",
      "[1024.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('a : ', a)\n",
    "print('d : ', d)\n",
    "print('a.grad : ', a.grad)\n",
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.7 Training Mode and Prediction Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(autograd.is_training()) \n",
    "\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.8 Summary**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MXNet provides an autograd package to automate the derivation process. <br>\n",
    "  To do so, we first attach gradients to variables, record the computation, and then run the backward function.<br><br>\n",
    "- We can detach gradients and pass head gradients to the backward function to control the part of the computation will be used in the backward function.<br><br>\n",
    "- The running modes of MXNet include the training mode and the prediction mode. <br>\n",
    "  We can determine the running mode by autograd.is_training()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.9 Exercises**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.3.9 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L Python3.7",
   "language": "python",
   "name": "d2l-py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
