{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 4 - THE PRELIMINARIES: A CRASHCOURSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Automatic Differentiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **autograd** package 는 자동으로 미분값을 계산해주고 backproaation 을 쉽게 할수 있도록 도와줌.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.1 A simple Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **MXnet 의 autograd 의 기본적인 사용법을 설명**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 예로서, 벡터 x 로 $y = 2x^\\top x$ 미분하려고 함. 벡터 x 를 초기화하고 할당함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 2. 3.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ndarray 의 attach_grad method 를 호출하여 기울기를 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 함수 불릴 때, x.grad 값 만들어짐. 호출 안하면 x.grad 값은 None\n",
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 y를 계산하고 MXnet은 바로 연산 그래프를 생성할 것이다. 마치 MXnet 이 레코딩 장치를 켜고 생성되는 변수들을 바로 캡쳐한것과 같다.<br>\n",
    "  계산 그래프를 만드는 것은 적지않은 계산양이 필요하다는 것을 주목해라.<br>\n",
    "  그렇기에 MXnet **_with autograd.record():_** block 안에서만 그래프를 만들 것이다.<br> \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[28.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * nd.dot(x, x)\n",
    "y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x 는 길이 4 인 vector 이고 nd.dot 이 inner product 룰 수핼할 것이다. 따라서 y 는 scalar 값이 나온다<br>\n",
    "  다음으로 우리는 **backward** function 을 호출하므로 모든 input 의 기울기를 자동적으로 찾을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x 에 대한 함수 $y = 2x^\\top x$ 의 기울기는 4x 이여야 한다. mxnet 에 의해 생성된 기울기가 맞는 값인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[ 0.  4.  8. 12.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)\n",
    "print(x.grad - 4 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만일 x 가 다른 부분에서 기울기 계산이 수행되었다면 이전의 **x.grad** 값은 덮어쓰여진다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.         0.26726124 0.5345225  0.80178374]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.norm()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.2 Backward for Non-scalar Variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y 가 scalar 가 아닐 경우 기울기는 고차원의 tensor 이고 계산이 복잡할 수 있다. <br>\n",
    "  다행히도 머신러닝과 딥러닝 모두에서, 종종 scalar 값이 되는 loss function 의 기울기 만을 계산한다.<br>\n",
    "  y 가 scalar 가 아닐 때, mxnet 은 기본적으로 새로운 변수를 얻기 위해 y 안에 element 를 합한 다음, 현재의 dydx 에서 x 에 대한 분석적 기울기를 찾을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y vector :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "v scalar :  \n",
      "[14.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "u.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record(): # y is a vector\n",
    "    y = x * x\n",
    "print('y vector : ', y)    \n",
    "y.backward()\n",
    "print('x.grad : ', x.grad)\n",
    "\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "\n",
    "with autograd.record(): # v is scalar\n",
    "    v = (u * u).sum()\n",
    "print('v scalar : ', v)    \n",
    "v.backward()\n",
    "print('u.grad : ', u.grad)\n",
    "\n",
    "x.grad - u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.3 Detach Computations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산 그래프의 일부를 계산 밖으로 옮길 수 있다.<br>\n",
    "  y = f(x), z = g(y) 를 가정했을 때, u = y.detach() 를 호출할 때, 이 동작은 새로운 변수를 리턴하고 u 가 어떻게 계산되었는지를 잊게함.<br>\n",
    "  u 를 constant 같이 취급함.<br>\n",
    "  아래의 backward 계산 예제는 x 에 대한 미분계산을 $∂x^3/∂x$ 대신에  $∂ux/∂x$ 로 실행함. <br>\n",
    "  $∂ux/∂x = u$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "z :  \n",
      "[ 0.  1.  8. 27.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상단에서 x.attach_grad() 호출되어 있는 상태.\n",
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "print('x : ', x)\n",
    "print('u : ', u)\n",
    "print('z : ', z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x.grad : ', x.grad)\n",
    "\n",
    "x.grad - u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y 의 계산은 여전히 기록되고 있으므로 우리는 y.backward 를 호출하여 $∂y/∂x = 2x$ 를 얻을 수 있다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "y :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "print('x : ', x)\n",
    "print('y : ', y)\n",
    "print('x.grad : ', x.grad)\n",
    "\n",
    "\n",
    "x.grad - 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 **detach** 함수를 통해 autograd.record scope 안에서 여러 단계의 계산이 있을 경우, 일부 미분 계산 선택적으로 하는 것이 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.4 Attach Gradients to Internal Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- attach grad 를 실행하면 내재적으로 detach 가 실행됨. <br>\n",
    "  아래 예제에서 u 를 다른 변수 기반으로 계산한다면 backward 계산에서 y 는 아예 사용되지 않게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "y : \n",
      "[2. 2. 2. 2.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u : \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "z : \n",
      "[0. 3. 6. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "x.grad : \n",
      "[1. 1. 1. 1.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "u.grad : \n",
      "[1. 1. 1. 1.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "y.grad : \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# x.attach_grad() 는 상단에서 호출되어 있음.\n",
    "\n",
    "y = nd.ones(4) * 2 \n",
    "print('x :', x)\n",
    "print('y :', y)\n",
    "\n",
    "y.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    \n",
    "    # implicitly run u = u.detach(), u = x * y 가 z.backward 에는 포함되지 않는다.\n",
    "    u.attach_grad() \n",
    "    \n",
    "    z = u + x \n",
    "    \n",
    "z.backward() # 실질적으로 'u + x' 에 대한 미분값 만을 계산함.\n",
    "print('u :', u)\n",
    "print('z :', z)\n",
    "\n",
    "print('\\nx.grad :', x.grad)\n",
    "print('\\nu.grad :', u.grad)\n",
    "print('\\ny.grad :', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.5 Head gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Detach 를 통해 여러 개의 계산식에 대한 각각의 미분값을 구할 수 있게 됨._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- u = f(x), z = g(u) 라고 가정했을 때 체인룰에 의해 다음과 같이 표현된다.<br>\n",
    "  $$\\frac{dz}{dx} = \\frac{dz}{du} \\frac{du}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{dz}{du}$ 를 구하기 위해, 우리는 먼저 u.detach 를 실행한 후 z.backward 를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "y : \n",
      "[2. 2. 2. 2.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "u = x * y : \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "z = u + x : \n",
      "[0. 3. 6. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "\n",
      "run `z.backward`\n",
      "\n",
      "\n",
      "x.grad :  \n",
      "[1. 1. 1. 1.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "y.grad :  \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "v.grad = dz/du = d(u + x)/du =  \n",
      "[1. 1. 1. 1.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y = nd.ones(4) * 2 \n",
    "\n",
    "print('x :', x)\n",
    "print('y :', y)\n",
    "\n",
    "y.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    v = u.detach() # u still keeps the computation graph\n",
    "    v.attach_grad()\n",
    "    z = v + x \n",
    "\n",
    "print('\\nu = x * y :', u)\n",
    "print('\\nz = u + x :', z)\n",
    "\n",
    "print('\\n\\nrun `z.backward`')\n",
    "z.backward() \n",
    "\n",
    "print('\\n\\nx.grad : ',x.grad)\n",
    "print('\\ny.grad : ',y.grad)\n",
    "print('\\nv.grad = dz/du = d(u + x)/du = ',v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/du =  \n",
      "[1. 1. 1. 1.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      " du/dx = d(x * y)/dx = y =  \n",
      "[2. 2. 2. 2.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      " \n",
      "\n",
      " x.grad --> dz/dx =  \n",
      "[2. 2. 2. 2.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "u.backward(v.grad) # v.grad 집어넣으면 결국 dz/du * du/dx = dz/dx 의 결과가 나옴. \n",
    "\n",
    "print('dz/du = ', v.grad)\n",
    "print('\\n du/dx = d(x * y)/dx = y = ', y)\n",
    "print('\\n ')\n",
    "print('\\n x.grad --> dz/dx = ', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.6 Computing the Gradient of Python Control Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    \n",
    "    while b.norm().asscalar() < 1000:\n",
    "        b = b * 2\n",
    "        \n",
    "    if b.sum().asscalar() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "        \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random.normal(shape=1) \n",
    "a.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    d = f(a) \n",
    "    \n",
    "d.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  \n",
      "[1.1630787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "d :  \n",
      "[1190.9926]\n",
      "<NDArray 1 @cpu(0)>\n",
      "a.grad :  \n",
      "[1024.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('a : ', a)\n",
    "print('d : ', d)\n",
    "print('a.grad : ', a.grad)\n",
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.7 Training Mode and Prediction Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(autograd.is_training()) \n",
    "\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.8 Summary**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MXNet provides an autograd package to automate the derivation process. <br>\n",
    "  To do so, we first attach gradients to variables, record the computation, and then run the backward function.<br><br>\n",
    "- We can detach gradients and pass head gradients to the backward function to control the part of the computation will be used in the backward function.<br><br>\n",
    "- The running modes of MXNet include the training mode and the prediction mode. <br>\n",
    "  We can determine the running mode by autograd.is_training()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L Python3.7",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
