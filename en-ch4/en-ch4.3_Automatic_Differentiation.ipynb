{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 4 - THE PRELIMINARIES: A CRASHCOURSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Automatic Differentiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **autograd** package 는 자동으로 미분값을 계산해주고 backproaation 을 쉽게 할수 있도록 도와줌.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.1 A simple Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **MXnet 의 autograd 의 기본적인 사용법을 설명**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 예로서, 벡터 x 로 $y = 2x^\\top x$ 미분하려고 함. 벡터 x 를 초기화하고 할당함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 2. 3.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ndarray 의 attach_grad method 를 호출하여 기울기를 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 y를 계산하고 MXnet은 바로 연산 그래프를 생성할 것이다. 마치 MXnet 이 레코딩 장치를 켜고 생성되는 변수들을 바로 캡쳐한것과 같다.<br>\n",
    "  계산 그래프를 만드는 것은 적지않은 계산양이 필요하다는 것을 주목해라.<br>\n",
    "  그렇기에 MXnet **_with autograd.record():_** block 안에서만 그래프를 만들 것이다.<br> \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[28.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * nd.dot(x, x)\n",
    "y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x 는 길이 4 인 vector 이고 nd.dot 이 inner product 룰 수핼할 것이다. 따라서 y 는 scalar 값이 나온다<br>\n",
    "  다음으로 우리는 **backward** function 을 호출하므로 모든 input 의 기울기를 자동적으로 찾을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x 에 대한 함수 $y = 2x^\\top x$ 의 기울기는 4x 이여야 한다. mxnet 에 의해 생성된 기울기가 맞는 값인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[ 0.  4.  8. 12.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)\n",
    "print(x.grad - 4 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만일 x 가 다른 부분에서 기울기 계산이 수행되었다면 이전의 **x.grad** 값은 덮어쓰여진다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.         0.26726124 0.5345225  0.80178374]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.norm()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.2 Backward for Non-scalar Variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y 가 scalar 가 아닐 경우 기울기는 고차원의 tensor 이고 계산이 복잡할 수 있다. <br>\n",
    "  다행히도 머신러닝과 딥러닝 모두에서, 종종 scalar 값이 되는 loss function 의 기울기 만을 계산한다.<br>\n",
    "  y 가 scalar 가 아닐 때, mxnet 은 기본적으로 새로운 변수를 얻기 위해 y 안에 element 를 합한 다음, 현재의 dydx 에서 x 에 대한 분석적 기울기를 찾을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y vector :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "v scalar :  \n",
      "[14.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "u.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record(): # y is a vector\n",
    "    y = x * x\n",
    "print('y vector : ', y)    \n",
    "y.backward()\n",
    "print('x.grad : ', x.grad)\n",
    "\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "\n",
    "with autograd.record(): # v is scalar\n",
    "    v = (u * u).sum()\n",
    "print('v scalar : ', v)    \n",
    "v.backward()\n",
    "print('u.grad : ', u.grad)\n",
    "\n",
    "x.grad - u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.3 Detach Computations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then u = y.detach() will return a new variable has the same values as y but forgets how u is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "z :  \n",
      "[ 0.  1.  8. 27.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "print('x : ', x)\n",
    "print('u : ', u)\n",
    "print('z : ', z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x.grad : ', x.grad)\n",
    "print('u : ', u)\n",
    "\n",
    "x.grad - u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following backward computes **_∂$u^2$x/∂x_** with u = x instead of **∂$x^3$/∂x**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the computation of y is still recorded, we can call y.backward() to get **∂y/∂x = 2x**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y :  \n",
      "[0. 1. 4. 9.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x.grad :  \n",
      "[0. 2. 4. 6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "x :  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "print('y : ', y)\n",
    "print('x.grad : ', x.grad)\n",
    "print('x : ', x)\n",
    "\n",
    "x.grad - 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.4 Attach Gradients to Internal Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attaching gradients to a variable x implicitly calls x=x.detach().<br>\n",
    "  If x is computed based on other variables, this part of computation will not be used in the backward function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 1. 1. 1.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [1. 1. 1. 1.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [0. 0. 0. 0.]\n",
       " <NDArray 4 @cpu(0)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.ones(4) * 2 \n",
    "y.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    u.attach_grad() # implicitly run u = u.detach()\n",
    "    z = u + x \n",
    "    \n",
    "z.backward() \n",
    "x.grad, u.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.5 Head gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [1. 1. 1. 1.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [0. 0. 0. 0.]\n",
       " <NDArray 4 @cpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.ones(4) * 2 \n",
    "y.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    v = u.detach() # u still keeps the computation graph\n",
    "    v.attach_grad()\n",
    "    z = v + x \n",
    "    \n",
    "z.backward() \n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [2. 2. 2. 2.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [0. 1. 2. 3.]\n",
       " <NDArray 4 @cpu(0)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.backward(v.grad) \n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.6 Computing the Gradient of Python Control Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    \n",
    "    while b.norm().asscalar() < 1000:\n",
    "        b = b * 2\n",
    "        \n",
    "    if b.sum().asscalar() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "        \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random.normal(shape=1) \n",
    "a.attach_grad() \n",
    "\n",
    "with autograd.record():\n",
    "    d = f(a) \n",
    "    \n",
    "d.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  \n",
      "[1.1630787]\n",
      "<NDArray 1 @cpu(0)>\n",
      "d :  \n",
      "[1190.9926]\n",
      "<NDArray 1 @cpu(0)>\n",
      "a.grad :  \n",
      "[1024.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('a : ', a)\n",
    "print('d : ', d)\n",
    "print('a.grad : ', a.grad)\n",
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.7 Training Mode and Prediction Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(autograd.is_training()) \n",
    "\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3.8 Summary**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MXNet provides an autograd package to automate the derivation process. <br>\n",
    "  To do so, we first attach gradients to variables, record the computation, and then run the backward function.<br><br>\n",
    "- We can detach gradients and pass head gradients to the backward function to control the part of the computation will be used in the backward function.<br><br>\n",
    "- The running modes of MXNet include the training mode and the prediction mode. <br>\n",
    "  We can determine the running mode by autograd.is_training()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L Study Py3.7",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
