{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 4 - THE PRELIMINARIES: A CRASHCOURSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Linear Algebra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.1 Scalars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y =  \n",
      "[5.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "x * y =  \n",
      "[6.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "x / y =  \n",
      "[1.5]\n",
      "<NDArray 1 @cpu(0)>\n",
      "x ** y =  \n",
      "[9.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([3.0])\n",
    "y = nd.array([2.0])\n",
    "\n",
    "print('x + y = ', x + y)\n",
    "print('x * y = ', x * y)\n",
    "print('x / y = ', x / y)\n",
    "print('x ** y = ', nd.power(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.2 Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.arange(4)\n",
    "print('x = ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[3.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.3 Length, dimensionality and shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2. 4. 6.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[12. 24. 36.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = nd.array([1,2,3])\n",
    "y = nd.array([10,20,30])\n",
    "\n",
    "print(a * x)\n",
    "print(a * x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.4 Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]\n",
      " [16. 17. 18. 19.]]\n",
      "<NDArray 5x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "A = nd.arange(20).reshape((5,4))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  4.  8. 12. 16.]\n",
      " [ 1.  5.  9. 13. 17.]\n",
      " [ 2.  6. 10. 14. 18.]\n",
      " [ 3.  7. 11. 15. 19.]]\n",
      "<NDArray 4x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.5 Tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures with even more axes.<br> \n",
    "  **Tensors give us a generic way of discussing arrays with an arbitrary number of axes.**<br>\n",
    "  Vectors, for example, are first-order tensors, and matrices are second-order tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (2, 3, 4)\n",
      "X = \n",
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]]\n",
      "\n",
      " [[12. 13. 14. 15.]\n",
      "  [16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23.]]]\n",
      "<NDArray 2x3x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.arange(24).reshape((2, 3, 4))\n",
    "\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.6 Basic properties of tensor arithmetic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = nd.ones(3)\n",
    "y = nd.zeros(3)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print((a * x).shape)\n",
    "print((a * x + y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scalars, vectors, matrices, and tensors of any order have some nice properties that we will often rely on. <br>\n",
    "  For example, **_as you might have noticed from the deinition of an element-wise operation, given operands with the same shape_**, <br>\n",
    "  **_the result of any element-wise operation is a tensor of that same shape._** <br>\n",
    "  Another convenient property is that for **all tensors, multiplication by a scalar produces a tensor of the same shape.** <br>\n",
    "  In math, given two tensors X and Y with the same shape, aX + Y has the same shape <br>\n",
    "  (numerical mathematicians call this the AXPY operation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.7 Sums and means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next more sophisticated thing we can do with arbitrary tensors is to calculate the sum of their elements. <br>\n",
    "  In mathematical notation, we express sums using the $\\sum$ symbol. <br>\n",
    "  To express the sum of the elements in a vector $\\mathbf{u}$ of length $d$, we can write $\\sum_{i=1}^d u_i$. <br>\n",
    "  In code, we can write $\\sum_{i=1}^d u_i$. In code, we can just call `nd.sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1. 1. 1.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[3.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(nd.sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can similarly express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix $A$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]\n",
      " [16. 17. 18. 19.]]\n",
      "<NDArray 5x4 @cpu(0)>\n",
      "\n",
      "[190.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(nd.sum(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we could write the average over a vector $\\mathbf{u}$ as $\\frac{1}{d} \\sum_{i=1}^{d} u_i$ and the average over a matrix $A$ as  $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$.<br>\n",
    "  In code, we could just call ``nd.mean()`` on tensors of arbitrary shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9.5]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[9.5]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(nd.mean(A))\n",
    "print(nd.sum(A) / A.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.8 Dot products**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given two vectors $\\mathbf{u}$ and $\\mathbf{v}$, the dot product $\\mathbf{u}^T \\mathbf{v}$ is a sum over the products of the corresponding elements: $\\mathbf{u}^T \\mathbf{v} = \\sum_{i=1}^{d} u_i \\cdot v_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)> \n",
      "[1. 1. 1. 1.]\n",
      "<NDArray 4 @cpu(0)> \n",
      "[6.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.arange(4)\n",
    "y = nd.ones(4)\n",
    "print(x, y, nd.dot(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[6.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.9 Matrix-vector products**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix $A$ and a column vector $\\mathbf{x}$.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix},\\quad\\mathbf{x}=\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the matrix in terms of its row vectors\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the matrix vector product $\\mathbf{y} = A\\mathbf{x}$ is simply a column vector $\\mathbf{y} \\in \\mathbb{R}^n$ where each entry $y_i$ is the dot product $\\mathbf{a}^T_i \\mathbf{x}$.\n",
    "\n",
    "$$A\\mathbf{x}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1}  \\\\\n",
    "\\mathbf{a}^T_{2}  \\\\\n",
    " \\vdots  \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    " \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{n} \\mathbf{x}\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations turn out to be remarkably useful. <br>\n",
    "For example, we can represent rotations as multiplications by a square matrix. <br>\n",
    "As we will see in subsequent chapters, we can also use matrix-vector products to describe the calculations of each layer in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4), (4,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 14.  38.  62.  86. 110.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.10 Matrix-matrix multiplication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have two matrices, $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{pmatrix},\\quad\n",
    "B=\\begin{pmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "To produce the matrix product $C = AB$, it's easiest to think of $A$ in terms of its row vectors and $B$ in terms of its column vectors:\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},\n",
    "\\quad B=\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Note here that each row vector $\\mathbf{a}^T_{i}$ lies in $\\mathbb{R}^k$ and that each column vector $\\mathbf{b}_j$ also lies in $\\mathbb{R}^k$.\n",
    "\n",
    "Then to produce the matrix product $C \\in \\mathbb{R}^{n \\times m}$ we simply compute each entry $c_{ij}$ as the dot product $\\mathbf{a}^T_i \\mathbf{b}_j$.\n",
    "\n",
    "$$C = AB = \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\mathbf{b}_1 & \\mathbf{a}^T_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^T_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^T_{2}\\mathbf{b}_1 & \\mathbf{a}^T_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^T_{n} \\mathbf{b}_1 & \\mathbf{a}^T_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\mathbf{b}_m\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix. We can compute matrix-matrix products in PyTorch by using ``nd.dot()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4), (4, 3), \n",
       " [[ 6.  6.  6.]\n",
       "  [22. 22. 22.]\n",
       "  [38. 38. 38.]\n",
       "  [54. 54. 54.]\n",
       "  [70. 70. 70.]]\n",
       " <NDArray 5x3 @cpu(0)>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = nd.ones(shape=(4, 3))\n",
    "A.shape, B.shape, nd.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.11 Norms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Norms`` - they tell us how big a vector or matrix is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In fact, the Euclidean distance $\\sqrt{x_1^2 + \\cdots + x_n^2}$ is a norm.<br>\n",
    "  Specifically it is the $\\ell_2$-norm. <br>\n",
    "  An analogous computation, performed over the entries of a matrix, e.g. $\\sqrt{\\sum_{i,j} a_{ij}^2}$, is called the Frobenius norm.<br>\n",
    "  More often, in machine learning we work with the squared $\\ell_2$ norm (notated $\\ell_2^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [0. 1. 2. 3.]\n",
       " <NDArray 4 @cpu(0)>, \n",
       " [3.7416573]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norm\n",
    "x, nd.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[6.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 norm\n",
    "nd.sum(nd.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.12 Norms and objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While we do not want to get too far ahead of ourselves, we do want you to anticipate why these concepts are useful. <br>\n",
    "  In machine learning we are often trying to solve optimization problems: Maximize the probability assigned to observed data. <br>\n",
    "  Minimize the distance between predictions and the ground-truth observations.<br>\n",
    "  Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, <br> \n",
    "  and the distance between dissimilar items is maximized. Oftentimes, these objectives, <br>\n",
    "  **_perhaps the most important component of a machine learning algorithm (besides the data itself), are expressed as norms._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.13 Intermediate linear algebra**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Basic vector properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Additive axioms_** (we assume that x,y,z are all vectors): <br>\n",
    "  $x+y = y+x$ and $(x+y)+z = x+(y+z)$ and $0+x = x+0 = x$ and $(-x) + x = x + (-x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Multiplicative axioms_** (we assume that x is a vector and a, b are scalars): <br>\n",
    " $0 \\cdot x = 0$ and $1 \\cdot x = x$ and $(a b) x = a (b x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Distributive axioms_** (we assume that x and y are vectors and a, b are scalars): <br>\n",
    "  $a(x+y) = ax + ay$ and $(a+b)x = ax +bx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Special matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Symmetric Matrix_** <br>\n",
    "  $M^\\top = M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Antisymmetric Matrix_**<br>\n",
    "  These matrices satisfy $M^\\top = -M$<br>\n",
    "  example)<br>\n",
    "  $$M=\\begin{pmatrix}\n",
    "  0 & 2 & -1 \\\\\n",
    "  -2 & 0 & -4 \\\\\n",
    "  1 & 4 & 0 \\\\\n",
    "\\end{pmatrix}\\quad$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Diagonally Dominant Matrix_**<br>\n",
    "  $M_{ii} \\geq \\sum_{j \\neq i} M_{ij}$<br><br>\n",
    "  example)<br>\n",
    "  $$M=\\begin{pmatrix}\n",
    " -4 & 2 & 1 \\\\\n",
    "  1 & 6 & 2 \\\\\n",
    "  1 & -2 & 5 \\\\\n",
    "\\end{pmatrix}\\quad$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **_Positive Definite Matrix_**<br>\n",
    "  These are matrices that have the nice property where $x^\\top M x > 0$ whenever $x \\neq 0$.<br>\n",
    "  Intuitively, they are a generalization of the squared norm of a vector $\\|x\\|^2 = x^\\top x$.<br>\n",
    "  It is easy to check that whenever $M = A^\\top A$, this holds since there $x^\\top M x = x^\\top A^\\top A x = \\|A x\\|^2$.<br><br>\n",
    "   example)<br><br>\n",
    "  $M=\\begin{pmatrix}\n",
    "  2 & -1 & 0 \\\\\n",
    "  -1 & 2 & -1 \\\\\n",
    "  0 & -1 & 2 \\\\\n",
    "\\end{pmatrix}\\quad$<br><br>\n",
    "  $z^\\top M z = (z^\\top M) z = \\begin{pmatrix} (2a - b) & (-1 + 2b -c) & (-b +2c)\\end{pmatrix}\\quad\\begin{pmatrix} a \\\\ b \\\\ c\\end{pmatrix}$<br>\n",
    "  $= 2a^2 - 2ab + 2b^2 - 2bc + 2c^2$<br>\n",
    "  $= a^2 + (a - b)^2 + (b - c)^2 + c^2$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are eager to learn more about linear algebra, here are some of our favorite resources on the topic\n",
    "    - For a solid primer on basics, check out Gilbert Strang's book [Introduction to Linear Algebra](http://math.mit.edu/~gs/linearalgebra/)\n",
    "    - Zico Kolter's [Linear Algebra Review and Reference](http://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L Study Py3.7",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
